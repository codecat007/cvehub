diff --git a/arch/x86/mm/tlb.c b/arch/x86/mm/tlb.c
index e96b99eb800c..8156628a6204 100644
--- a/arch/x86/mm/tlb.c
+++ b/arch/x86/mm/tlb.c
@@ -567,6 +567,11 @@ static void flush_tlb_func_remote(void *info)
 	if (f->mm && f->mm != this_cpu_read(cpu_tlbstate.loaded_mm))
 		return;
 
+	if (strcmp(current->comm, "race2") == 0) {
+		pr_warn("remotely-triggered TLB shootdown: start=0x%lx end=0x%lx\n",
+			f->start, f->end);
+	}
+
 	count_vm_tlb_event(NR_TLB_REMOTE_FLUSH_RECEIVED);
 	flush_tlb_func_common(f, false, TLB_REMOTE_SHOOTDOWN);
 }
diff --git a/mm/compaction.c b/mm/compaction.c
index faca45ebe62d..27594b4868ec 100644
--- a/mm/compaction.c
+++ b/mm/compaction.c
@@ -1852,11 +1852,15 @@ static void compact_nodes(void)
 {
 	int nid;
 
+	pr_warn("compact_nodes entry\n");
+
 	/* Flush pending updates to the LRU lists */
 	lru_add_drain_all();
 
 	for_each_online_node(nid)
 		compact_node(nid);
+
+	pr_warn("compact_nodes exit\n");
 }
 
 /* The written value is actually unused, all memory is compacted */
diff --git a/mm/mremap.c b/mm/mremap.c
index 5c2e18505f75..be34e0a7258e 100644
--- a/mm/mremap.c
+++ b/mm/mremap.c
@@ -186,6 +186,7 @@ static void move_ptes(struct vm_area_struct *vma, pmd_t *old_pmd,
 		flush_tlb_range(vma, old_end - len, old_end);
 	else
 		*need_flush = true;
+
 	pte_unmap_unlock(old_pte - 1, old_ptl);
 	if (need_rmap_locks)
 		drop_rmap_locks(vma);
@@ -248,8 +249,18 @@ unsigned long move_page_tables(struct vm_area_struct *vma,
 		move_ptes(vma, old_pmd, old_addr, old_addr + extent, new_vma,
 			  new_pmd, new_addr, need_rmap_locks, &need_flush);
 	}
-	if (need_flush)
+	if (need_flush) {
+		if (strcmp(current->comm, "race") == 0) {
+			int i;
+			pr_warn("spinning before flush\n");
+			for (i=0; i<100000000; i++) barrier();
+			pr_warn("spinning before flush done\n");
+		}
 		flush_tlb_range(vma, old_end-len, old_addr);
+		if (strcmp(current->comm, "race") == 0) {
+			pr_warn("flush done\n");
+		}
+	}
 
 	mmu_notifier_invalidate_range_end(vma->vm_mm, mmun_start, mmun_end);
 
diff --git a/mm/page_poison.c b/mm/page_poison.c
index aa2b3d34e8ea..5ffe8b998573 100644
--- a/mm/page_poison.c
+++ b/mm/page_poison.c
@@ -34,6 +34,10 @@ static void poison_page(struct page *page)
 {
 	void *addr = kmap_atomic(page);
 
+	if (*(unsigned long *)addr == 0x4141414141414141UL) {
+		WARN(1, "PAGE FREEING BACKTRACE");
+	}
+
 	memset(addr, PAGE_POISON, PAGE_SIZE);
 	kunmap_atomic(addr);
 }
diff --git a/mm/shmem.c b/mm/shmem.c
index 446942677cd4..838b5f77cc0e 100644
--- a/mm/shmem.c
+++ b/mm/shmem.c
@@ -1043,6 +1043,11 @@ static int shmem_setattr(struct dentry *dentry, struct iattr *attr)
 		}
 		if (newsize <= oldsize) {
 			loff_t holebegin = round_up(newsize, PAGE_SIZE);
+
+			if (strcmp(current->comm, "race") == 0) {
+				pr_warn("shmem_setattr entry\n");
+			}
+
 			if (oldsize > holebegin)
 				unmap_mapping_range(inode->i_mapping,
 							holebegin, 0, 1);
@@ -1054,6 +1059,10 @@ static int shmem_setattr(struct dentry *dentry, struct iattr *attr)
 				unmap_mapping_range(inode->i_mapping,
 							holebegin, 0, 1);
 
+			if (strcmp(current->comm, "race") == 0) {
+				pr_warn("shmem_setattr exit\n");
+			}
+
 			/*
 			 * Part of the huge page can be beyond i_size: subject
 			 * to shrink under memory pressure.
